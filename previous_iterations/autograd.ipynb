{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Proste API które można wykorzystać do liczenia automatycznego gradientu\n",
    "2. Zaimplementowanie za pomocą tego API aproksymacji jakiejś prostej funkcji / regresji liniowej\n",
    "3. Zaimplementowanie za pomocą tego API sieci neuronowej \n",
    "4. Zaimplementowanie tego samego za pomocą API autogradu\n",
    "5. Dodanie ulepszeń naszego API w taki sposób, żeby było bardziej podobne do tego od autogradu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this [great talk](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. \n",
    "So basically, the goal for this notebook would be to create a a code for which a gradient can be calculated by applying a simple rule. \n",
    "\n",
    "The logic would be the following: \n",
    "1. Create a Node containing information about parents and about current function\n",
    "2. Wrap every numpy function in wrapper function that would recieve an input, unbox it, calculate its value, and box it again\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun: 72.0, deriv: [12. 12. 12.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m grad_fun \u001b[39m=\u001b[39m grad(fun)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFun: \u001b[39m\u001b[39m{\u001b[39;00mfun(X, Y)\u001b[39m}\u001b[39;00m\u001b[39m, deriv: \u001b[39m\u001b[39m{\u001b[39;00mgrad_fun(X, Y)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(g(X, Y))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "# GOAL\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp\n",
    "import numpy as np\n",
    "\n",
    "def fun(X, Y):\n",
    "    return anp.sum(X) * anp.sum(Y)\n",
    "\n",
    "X = np.array([1.0,2.0,3.0])\n",
    "Y = np.array([3.0,4.0,5.0])\n",
    "\n",
    "grad_fun = grad(fun)\n",
    "\n",
    "print(f'Fun: {fun(X, Y)}, deriv: {grad_fun(X, Y)}')\n",
    "\n",
    "\n",
    "print(g(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from numpy.typing import NDArray\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, value: NDArray, parents: Iterable[Node], primitive) -> None:\n",
    "        \"\"\"\n",
    "        :param value: value associated with the node\n",
    "        :param parents: parents of the Node\n",
    "        :param primitive: function by which this node was produced\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.parents = parents\n",
    "        self.primitive = primitive\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we define the primitive class which will work as a wrapper for all operations that we can possibly perform and will help us build a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "from typing import Callable\n",
    "from numpy.typing import NDArray\n",
    "import operator as op\n",
    "\n",
    "\n",
    "class Primitive(ABC):\n",
    "    def __init__(self, fun: Callable[[NDArray], NDArray], argcount: int) -> None:\n",
    "        self.fun = fun\n",
    "        self._argc = argcount\n",
    "\n",
    "    def __call__(self, *args: Node) -> Node:\n",
    "        values = [node.value for node in args] \n",
    "        print(values)\n",
    "        primitive_results = self.fun(*values)\n",
    "        return Node(primitive_results, args, self)\n",
    "\n",
    "    def grad(self, argnum) -> Callable:\n",
    "        return partial(self._gradfun, argnum=argnum)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _gradfun(self, *args: Node, argnum):\n",
    "        ...\n",
    "\n",
    "class sum(Primitive):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(np.sum, 1)\n",
    "\n",
    "    def _gradfun(self, *args: Node, argnum):\n",
    "        arg = args[argnum].value\n",
    "        return np.ones(arg.shape)\n",
    "\n",
    "class product(Primitive):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(np.prod, 1)\n",
    "\n",
    "    \n",
    "    def _gradfun(self, *args: Node, argnum):\n",
    "        factors = [a for i, a in enumerate(args) if i != argnum] # Derivative of a product is simply a factor before the product\n",
    "        return np.prod(factors, axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "class value(Primitive):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(lambda x: x, 1)\n",
    "    \n",
    "    def _gradfun(self, *args: Node, argnum):\n",
    "        arg = args[argnum]\n",
    "        return np.zeros_like(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "\n",
    "def toposort(end_node, parents=operator.attrgetter('parents')):\n",
    "    child_counts = {}\n",
    "    stack = [end_node]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node in child_counts:\n",
    "            child_counts[node] += 1\n",
    "        else:\n",
    "            child_counts[node] = 1\n",
    "            stack.extend(parents(node))\n",
    "\n",
    "    childless_nodes = [end_node]\n",
    "    while childless_nodes:\n",
    "        node = childless_nodes.pop()\n",
    "        yield node\n",
    "        for parent in parents(node):\n",
    "            if child_counts[parent] == 1:\n",
    "                childless_nodes.append(parent)\n",
    "            else:\n",
    "                child_counts[parent] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(fun: Primitive, args):\n",
    "    start = Node(args, [], value)\n",
    "    end = fun(*args)\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast \n",
    "\n",
    "def backward_pass(gradient, end_node: Node):\n",
    "    \"\"\"\n",
    "    Retursn final grad and all intermediate grads\n",
    "    \"\"\"\n",
    "    outgrads = { end_node: gradient }\n",
    "    for node in toposort(end_node): # for all nodes coming into this node\n",
    "        outgrad = outgrads[node]\n",
    "        parents = node.parents\n",
    "        for p in parents:\n",
    "            primitive = cast(Primitive, p.primitive)\n",
    "            outgrads[p] = [\n",
    "                outgrad @ primitive.grad(argnum=i) \n",
    "                for i in range(primitive.argnum)]\n",
    "    return outgrad\n",
    "\n",
    "\n",
    "\n",
    "def grad(fun: Primitive, argnum=0):\n",
    "    \"\"\"\n",
    "    :param fun: primitive to be differentiated\n",
    "    :param argnum: number of arguments with respect to which \n",
    "    derivatives should be calculated\n",
    "    \"\"\"\n",
    "    def gradient_function(*args):\n",
    "        \"\"\"\n",
    "        returns change vector (jacobian) for every argument \n",
    "        in function for provided values\n",
    "        \"\"\"\n",
    "        _, end = forward_pass(fun, args)\n",
    "        return lambda grad_value: backward_pass(grad_value, end)\n",
    "        \n",
    "    return gradient_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nie komplikujemy. Skoro działamy na kombinacji funkcji, to działajmy na kombinacji funkcji :) \n",
    "\n",
    "# sum(X) * sum(X) -> product(sum(), sum())\n",
    "# Działajmy na węzłach\n",
    "\n",
    "\n",
    "# Primitive - is just a function, that can have arguments.\n",
    "\n",
    "\n",
    "X = value()\n",
    "Y = value()\n",
    "\n",
    "A = sum(X)\n",
    "B = sum(Y)\n",
    "\n",
    "C = product(A, B)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Funkcja zadaniem której jest wywołanie przekazanego \n",
    "# primitive z odpowiednimi argumentami\n",
    "def forward_pass(primitive: Primitive, *args: NDArray):\n",
    "    start_nodes = [Node(arg, [], value) for arg in args]\n",
    "    # TODO:\n",
    "    # - build a graph\n",
    "    # - \n",
    "    # Primitive - function \n",
    "    # Node - operation \n",
    "    # Nodes contain primitives\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# In order to call a primitive, we should: \n",
    "# 1. Find the very first primitives in the chain (i.e. one that does not have parents)\n",
    "# 2. Pass values to them, collect their responses and pass forward\n",
    "# 3. It actually should be done not in forward pass method, but in call of each primitive\n",
    "# 4. The most simple primitive would look like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3])]\n",
      "6\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m grads \u001b[39m=\u001b[39m [A\u001b[39m.\u001b[39mgrad(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m)]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     g(X) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ])\n",
      "\u001b[1;32m/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb Cell 12\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m grads \u001b[39m=\u001b[39m [A\u001b[39m.\u001b[39mgrad(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m([\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     g(X) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ])\n",
      "\u001b[1;32m/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb Cell 12\u001b[0m in \u001b[0;36msum._gradfun\u001b[0;34m(self, argnum, *args)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gradfun\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Node, argnum):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     arg \u001b[39m=\u001b[39m args[argnum]\u001b[39m.\u001b[39mvalue\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bohdan/projects/glider/ai-course/neural-nets/autograd.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mones(arg\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "A = sum()\n",
    "\n",
    "X = Node(np.array([1,2,3]), [], value)\n",
    "\n",
    "\n",
    "output = A(X)\n",
    "print(output.value)\n",
    "\n",
    "\n",
    "grads = [A.grad(i) for i in range(2)]\n",
    "print([\n",
    "    g(X) for g in grads\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd.test_util import check_grads\n",
    "\n",
    "\n",
    "def training_loss(weights):\n",
    "    return anp.sum(weights)\n",
    "\n",
    "\n",
    "inputs = np.array([[0.52, 1.12,  0.77]])\n",
    "\n",
    "\n",
    "\n",
    "training_gradient_fun = grad(training_loss)\n",
    "training_gradient_fun(weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('flatland')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce23526395cc72b0807a667c67a2b24d90b976cf5ecfddc37c967d302ca2994b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
