{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 265516.43417626707\n",
      "100 loss: 583.7630371292918\n",
      "200 loss: 389.168024081048\n",
      "300 loss: 260.44373957339354\n",
      "400 loss: 175.28984128106464\n",
      "500 loss: 118.95708608270887\n",
      "600 loss: 81.68954655168154\n",
      "700 loss: 57.03399987724072\n",
      "800 loss: 40.72175547114519\n",
      "900 loss: 29.929082485577982\n",
      "1000 loss: 22.78803946192646\n",
      "1100 loss: 18.06291984172044\n",
      "1200 loss: 14.93623746384328\n",
      "1300 loss: 12.867164124422818\n",
      "1400 loss: 11.497889326579992\n",
      "1500 loss: 10.591677962266244\n",
      "1600 loss: 9.991894891127654\n",
      "1700 loss: 9.594898577634943\n",
      "1800 loss: 9.332108966738467\n",
      "1900 loss: 9.158144176454114\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(-pi, pi, num=2000)\n",
    "y = np.sin(X)\n",
    "\n",
    "W = np.random.rand(4)\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "for step in range(2000):\n",
    "    \n",
    "    # polynomial \n",
    "    y_pred = W[0] + W[1] * X + W[2] * X ** 2 + W[3] * X ** 3\n",
    "\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if step % 100 == 0:\n",
    "        print(f'{step} loss: {loss}')\n",
    "\n",
    "    loss_grad = 2 * (y_pred - y)\n",
    "\n",
    "    grad = np.array([\n",
    "        loss_grad.sum(),\n",
    "        (X * loss_grad).sum(),\n",
    "        (X ** 2 * loss_grad).sum() ,\n",
    "        (X ** 3 * loss_grad).sum()\n",
    "    ])\n",
    "\n",
    "    W -= alpha * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we can use numpy and its quite easy. But what if we will want to create something more complex.\n",
    "The first uncofortable thing to notice is that we should calculate gradients manually. That is not cool. Let's try to change it by creating ou own classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "x = torch.linspace(-pi, pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "power = torch.tensor([1,2,3])\n",
    "polynomial = x.unsqueeze(-1).pow(power)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "for step in range(2000):\n",
    "    y_pred = model(polynomial)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= alpha * param.grad\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 495570.0625\n",
      "100 loss: 6180.21337890625\n",
      "200 loss: 4090.698486328125\n",
      "300 loss: 2708.70263671875\n",
      "400 loss: 1794.640869140625\n",
      "500 loss: 1190.0643310546875\n",
      "600 loss: 790.1793212890625\n",
      "700 loss: 525.6786499023438\n",
      "800 loss: 350.72332763671875\n",
      "900 loss: 234.99557495117188\n",
      "1000 loss: 158.4435577392578\n",
      "1100 loss: 107.80406951904297\n",
      "1200 loss: 74.30517578125\n",
      "1300 loss: 52.144309997558594\n",
      "1400 loss: 37.48378372192383\n",
      "1500 loss: 27.784563064575195\n",
      "1600 loss: 21.367605209350586\n",
      "1700 loss: 17.12200355529785\n",
      "1800 loss: 14.312911987304688\n",
      "1900 loss: 12.454136848449707\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "X = torch.linspace(-pi, pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(X)\n",
    "\n",
    "W = torch.randn((4,), device=device, dtype=dtype)\n",
    "\n",
    "alpha = 1e-6\n",
    "\n",
    "for step in range(2000):\n",
    "    \n",
    "    # polynomial \n",
    "    y_pred = W[0] + W[1] * X + W[2] * X ** 2 + W[3] * X ** 3\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if step % 100 == 0:\n",
    "        print(f'{step} loss: {loss}')\n",
    "\n",
    "    loss_grad = 2 * (y_pred - y)\n",
    "\n",
    "    grad = torch.tensor([\n",
    "        loss_grad.sum(),\n",
    "        (X * loss_grad).sum(),\n",
    "        (X ** 2 * loss_grad).sum() ,\n",
    "        (X ** 3 * loss_grad).sum()\n",
    "    ])\n",
    "\n",
    "    W -= alpha * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Ok, so we know what library would be used for learning students. \n",
    "But the actual question is to how make it interesting and fun. \n",
    "So let's make find some dataset.\n",
    "* [Game of life](https://www.kaggle.com/competitions/conway-s-reverse-game-of-life)\n",
    "* [Bicycle demand](https://www.kaggle.com/competitions/bike-sharing-demand) -- this one is actually cool, because the similar problem could be created for an RL lab and the difference between approaches would be clearly seen. \n",
    "* [Predicting an author](https://www.kaggle.com/competitions/painter-by-numbers) -- actually I wanted to do it. So the question is why not.\n",
    "\n",
    "\n",
    "# Objectives\n",
    "\n",
    "1, chcemy zrobić ten moduł w notatniku \n",
    "2, chcemy pokazać sieć neuronową jako kompozycję małych funkcji z których ona się składa (forward_pass, backward_pass, etc.)\n",
    "3. chcemy pokazać podejście od dwóch stron: czyli najpierw jak trzeba się napracować, żeby zrobić coś za pomocą numpy, a potem jak łatwo da się to zrobić za pomocą pytorcha (albo innej podobnej biblioteki, która ma opcję liczenia gradientów)\n",
    "4. Jako punkt startowy możemy użyć tego poradnika od pytorch - https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "--- od mnie \n",
    "5. Cause -> effect tego dlaczego API jest taki jaki jest i dlaczego sieci neuronowe działają\n",
    "\n",
    "\n",
    "# What problems do I have with neural networks, because of how I was (wasn't) taught about NN\n",
    "\n",
    "1. I don't know nothing about different types of pooling and it's benefits\n",
    "2. I am not sure why we should initialize randomized weights\n",
    "3. I don't know nothing about different types of activation functions and their benefits\n",
    "4. I don't know how people actually compare different types of nets\n",
    "5. I don't know nothing about different types of optimization algorithms \n",
    "6. I don't know nothing about computation graphs and why they are cool, and how they are built.\n",
    "7. I am not sure about epochs and batches \n",
    "\n",
    "\n",
    "# Plan\n",
    "\n",
    "I want to focus on the computation graph problem, because it appears in both pytorch and tensorflow. \n",
    "\n",
    "I will go from matrices and sinus function to explain the high level concept. \n",
    "After that, I am planning to go tep further and introduce the bicycle dataset. \n",
    "It would be harder to do with simple matrices, so instead we will need to create \n",
    "a more complicated PyTorch-like api. \n",
    "\n",
    "The last step would be to introduce a Pytorch and show that it is actually that simple.\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "1. Learn about computational graph in pytorch and it benefits \n",
    "2. Try to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) loss - 167209.68792349967\n",
      "(100) loss - 17.830195949074362\n",
      "(200) loss - 12.81887445316103\n",
      "(300) loss - 10.59396094800275\n",
      "(400) loss - 9.60607893415918\n",
      "(500) loss - 9.167450254425438\n",
      "(600) loss - 8.972695095419748\n",
      "(700) loss - 8.886222017593761\n",
      "(800) loss - 8.847827177493617\n",
      "(900) loss - 8.830779516147715\n",
      "(1000) loss - 8.823210198067304\n",
      "(1100) loss - 8.819849351377545\n",
      "(1200) loss - 8.818357104605507\n",
      "(1300) loss - 8.817694533344689\n",
      "(1400) loss - 8.817400345624598\n",
      "(1500) loss - 8.817269723577184\n",
      "(1600) loss - 8.817211726189438\n",
      "(1700) loss - 8.81718597481705\n",
      "(1800) loss - 8.817174540971395\n",
      "(1900) loss - 8.817169464238997\n",
      "(2000) loss - 8.817167210123223\n",
      "(2100) loss - 8.81716620927514\n",
      "(2200) loss - 8.817165764889403\n",
      "(2300) loss - 8.817165567578057\n",
      "(2400) loss - 8.81716547997001\n",
      "(2500) loss - 8.817165441071232\n",
      "(2600) loss - 8.817165423799818\n",
      "(2700) loss - 8.81716541613115\n",
      "(2800) loss - 8.817165412726192\n",
      "(2900) loss - 8.817165411214361\n",
      "(3000) loss - 8.817165410543094\n",
      "(3100) loss - 8.817165410245043\n",
      "(3200) loss - 8.817165410112707\n",
      "(3300) loss - 8.81716541005395\n",
      "(3400) loss - 8.817165410027862\n",
      "(3500) loss - 8.817165410016278\n",
      "(3600) loss - 8.817165410011134\n",
      "(3700) loss - 8.817165410008847\n",
      "(3800) loss - 8.817165410007833\n",
      "(3900) loss - 8.817165410007384\n",
      "(4000) loss - 8.817165410007185\n",
      "(4100) loss - 8.817165410007096\n",
      "(4200) loss - 8.817165410007057\n",
      "(4300) loss - 8.81716541000704\n",
      "(4400) loss - 8.817165410007032\n",
      "(4500) loss - 8.817165410007028\n",
      "(4600) loss - 8.817165410007028\n",
      "(4700) loss - 8.817165410007025\n",
      "(4800) loss - 8.817165410007021\n",
      "(4900) loss - 8.817165410007025\n",
      "(5000) loss - 8.817165410007025\n",
      "(5100) loss - 8.817165410007023\n",
      "(5200) loss - 8.817165410007023\n",
      "(5300) loss - 8.817165410007027\n",
      "(5400) loss - 8.817165410007025\n",
      "(5500) loss - 8.817165410007025\n",
      "(5600) loss - 8.817165410007028\n",
      "(5700) loss - 8.817165410007023\n",
      "(5800) loss - 8.817165410007025\n",
      "(5900) loss - 8.817165410007023\n",
      "(6000) loss - 8.817165410007025\n",
      "(6100) loss - 8.817165410007025\n",
      "(6200) loss - 8.817165410007025\n",
      "(6300) loss - 8.817165410007025\n",
      "(6400) loss - 8.817165410007023\n",
      "(6500) loss - 8.817165410007025\n",
      "(6600) loss - 8.817165410007021\n",
      "(6700) loss - 8.817165410007025\n",
      "(6800) loss - 8.817165410007025\n",
      "(6900) loss - 8.817165410007025\n",
      "(7000) loss - 8.817165410007028\n",
      "(7100) loss - 8.817165410007023\n",
      "(7200) loss - 8.817165410007025\n",
      "(7300) loss - 8.817165410007025\n",
      "(7400) loss - 8.817165410007025\n",
      "(7500) loss - 8.817165410007023\n",
      "(7600) loss - 8.817165410007025\n",
      "(7700) loss - 8.817165410007025\n",
      "(7800) loss - 8.817165410007025\n",
      "(7900) loss - 8.817165410007025\n",
      "(8000) loss - 8.817165410007025\n",
      "(8100) loss - 8.817165410007025\n",
      "(8200) loss - 8.817165410007025\n",
      "(8300) loss - 8.817165410007025\n",
      "(8400) loss - 8.817165410007025\n",
      "(8500) loss - 8.817165410007025\n",
      "(8600) loss - 8.817165410007025\n",
      "(8700) loss - 8.817165410007025\n",
      "(8800) loss - 8.817165410007025\n",
      "(8900) loss - 8.817165410007025\n",
      "(9000) loss - 8.817165410007025\n",
      "(9100) loss - 8.817165410007025\n",
      "(9200) loss - 8.817165410007025\n",
      "(9300) loss - 8.817165410007025\n",
      "(9400) loss - 8.817165410007025\n",
      "(9500) loss - 8.817165410007025\n",
      "(9600) loss - 8.817165410007025\n",
      "(9700) loss - 8.817165410007025\n",
      "(9800) loss - 8.817165410007025\n",
      "(9900) loss - 8.817165410007025\n",
      "(10000) loss - 8.817165410007025\n",
      "(10100) loss - 8.817165410007025\n",
      "(10200) loss - 8.817165410007025\n",
      "(10300) loss - 8.817165410007025\n",
      "(10400) loss - 8.817165410007025\n",
      "(10500) loss - 8.817165410007025\n",
      "(10600) loss - 8.817165410007025\n",
      "(10700) loss - 8.817165410007025\n",
      "(10800) loss - 8.817165410007025\n",
      "(10900) loss - 8.817165410007025\n",
      "(11000) loss - 8.817165410007025\n",
      "(11100) loss - 8.817165410007025\n",
      "(11200) loss - 8.817165410007025\n",
      "(11300) loss - 8.817165410007025\n",
      "(11400) loss - 8.817165410007025\n",
      "(11500) loss - 8.817165410007025\n",
      "(11600) loss - 8.817165410007025\n",
      "(11700) loss - 8.817165410007025\n",
      "(11800) loss - 8.817165410007025\n",
      "(11900) loss - 8.817165410007025\n",
      "(12000) loss - 8.817165410007025\n",
      "(12100) loss - 8.817165410007025\n",
      "(12200) loss - 8.817165410007025\n",
      "(12300) loss - 8.817165410007025\n",
      "(12400) loss - 8.817165410007025\n",
      "(12500) loss - 8.817165410007025\n",
      "(12600) loss - 8.817165410007025\n",
      "(12700) loss - 8.817165410007025\n",
      "(12800) loss - 8.817165410007025\n",
      "(12900) loss - 8.817165410007025\n",
      "(13000) loss - 8.817165410007025\n",
      "(13100) loss - 8.817165410007025\n",
      "(13200) loss - 8.817165410007025\n",
      "(13300) loss - 8.817165410007025\n",
      "(13400) loss - 8.817165410007025\n",
      "(13500) loss - 8.817165410007025\n",
      "(13600) loss - 8.817165410007025\n",
      "(13700) loss - 8.817165410007025\n",
      "(13800) loss - 8.817165410007025\n",
      "(13900) loss - 8.817165410007025\n",
      "(14000) loss - 8.817165410007025\n",
      "(14100) loss - 8.817165410007025\n",
      "(14200) loss - 8.817165410007025\n",
      "(14300) loss - 8.817165410007025\n",
      "(14400) loss - 8.817165410007025\n",
      "(14500) loss - 8.817165410007025\n",
      "(14600) loss - 8.817165410007025\n",
      "(14700) loss - 8.817165410007025\n",
      "(14800) loss - 8.817165410007025\n",
      "(14900) loss - 8.817165410007025\n",
      "(15000) loss - 8.817165410007025\n",
      "(15100) loss - 8.817165410007025\n",
      "(15200) loss - 8.817165410007025\n",
      "(15300) loss - 8.817165410007025\n",
      "(15400) loss - 8.817165410007025\n",
      "(15500) loss - 8.817165410007025\n",
      "(15600) loss - 8.817165410007025\n",
      "(15700) loss - 8.817165410007025\n",
      "(15800) loss - 8.817165410007025\n",
      "(15900) loss - 8.817165410007025\n",
      "(16000) loss - 8.817165410007025\n",
      "(16100) loss - 8.817165410007025\n",
      "(16200) loss - 8.817165410007025\n",
      "(16300) loss - 8.817165410007025\n",
      "(16400) loss - 8.817165410007025\n",
      "(16500) loss - 8.817165410007025\n",
      "(16600) loss - 8.817165410007025\n",
      "(16700) loss - 8.817165410007025\n",
      "(16800) loss - 8.817165410007025\n",
      "(16900) loss - 8.817165410007025\n",
      "(17000) loss - 8.817165410007025\n",
      "(17100) loss - 8.817165410007025\n",
      "(17200) loss - 8.817165410007025\n",
      "(17300) loss - 8.817165410007025\n",
      "(17400) loss - 8.817165410007025\n",
      "(17500) loss - 8.817165410007025\n",
      "(17600) loss - 8.817165410007025\n",
      "(17700) loss - 8.817165410007025\n",
      "(17800) loss - 8.817165410007025\n",
      "(17900) loss - 8.817165410007025\n",
      "(18000) loss - 8.817165410007025\n",
      "(18100) loss - 8.817165410007025\n",
      "(18200) loss - 8.817165410007025\n",
      "(18300) loss - 8.817165410007025\n",
      "(18400) loss - 8.817165410007025\n",
      "(18500) loss - 8.817165410007025\n",
      "(18600) loss - 8.817165410007025\n",
      "(18700) loss - 8.817165410007025\n",
      "(18800) loss - 8.817165410007025\n",
      "(18900) loss - 8.817165410007025\n",
      "(19000) loss - 8.817165410007025\n",
      "(19100) loss - 8.817165410007025\n",
      "(19200) loss - 8.817165410007025\n",
      "(19300) loss - 8.817165410007025\n",
      "(19400) loss - 8.817165410007025\n",
      "(19500) loss - 8.817165410007025\n",
      "(19600) loss - 8.817165410007025\n",
      "(19700) loss - 8.817165410007025\n",
      "(19800) loss - 8.817165410007025\n",
      "(19900) loss - 8.817165410007025\n",
      "(20000) loss - 8.817165410007025\n",
      "(20100) loss - 8.817165410007025\n",
      "(20200) loss - 8.817165410007025\n",
      "(20300) loss - 8.817165410007025\n",
      "(20400) loss - 8.817165410007025\n",
      "(20500) loss - 8.817165410007025\n",
      "(20600) loss - 8.817165410007025\n",
      "(20700) loss - 8.817165410007025\n",
      "(20800) loss - 8.817165410007025\n",
      "(20900) loss - 8.817165410007025\n",
      "(21000) loss - 8.817165410007025\n",
      "(21100) loss - 8.817165410007025\n",
      "(21200) loss - 8.817165410007025\n",
      "(21300) loss - 8.817165410007025\n",
      "(21400) loss - 8.817165410007025\n",
      "(21500) loss - 8.817165410007025\n",
      "(21600) loss - 8.817165410007025\n",
      "(21700) loss - 8.817165410007025\n",
      "(21800) loss - 8.817165410007025\n",
      "(21900) loss - 8.817165410007025\n",
      "(22000) loss - 8.817165410007025\n",
      "(22100) loss - 8.817165410007025\n",
      "(22200) loss - 8.817165410007025\n",
      "(22300) loss - 8.817165410007025\n",
      "(22400) loss - 8.817165410007025\n",
      "(22500) loss - 8.817165410007025\n",
      "(22600) loss - 8.817165410007025\n",
      "(22700) loss - 8.817165410007025\n",
      "(22800) loss - 8.817165410007025\n",
      "(22900) loss - 8.817165410007025\n",
      "(23000) loss - 8.817165410007025\n",
      "(23100) loss - 8.817165410007025\n",
      "(23200) loss - 8.817165410007025\n",
      "(23300) loss - 8.817165410007025\n",
      "(23400) loss - 8.817165410007025\n",
      "(23500) loss - 8.817165410007025\n",
      "(23600) loss - 8.817165410007025\n",
      "(23700) loss - 8.817165410007025\n",
      "(23800) loss - 8.817165410007025\n",
      "(23900) loss - 8.817165410007025\n",
      "(24000) loss - 8.817165410007025\n",
      "(24100) loss - 8.817165410007025\n",
      "(24200) loss - 8.817165410007025\n",
      "(24300) loss - 8.817165410007025\n",
      "(24400) loss - 8.817165410007025\n",
      "(24500) loss - 8.817165410007025\n",
      "(24600) loss - 8.817165410007025\n",
      "(24700) loss - 8.817165410007025\n",
      "(24800) loss - 8.817165410007025\n",
      "(24900) loss - 8.817165410007025\n",
      "(25000) loss - 8.817165410007025\n",
      "(25100) loss - 8.817165410007025\n",
      "(25200) loss - 8.817165410007025\n",
      "(25300) loss - 8.817165410007025\n",
      "(25400) loss - 8.817165410007025\n",
      "(25500) loss - 8.817165410007025\n",
      "(25600) loss - 8.817165410007025\n",
      "(25700) loss - 8.817165410007025\n",
      "(25800) loss - 8.817165410007025\n",
      "(25900) loss - 8.817165410007025\n",
      "(26000) loss - 8.817165410007025\n",
      "(26100) loss - 8.817165410007025\n",
      "(26200) loss - 8.817165410007025\n",
      "(26300) loss - 8.817165410007025\n",
      "(26400) loss - 8.817165410007025\n",
      "(26500) loss - 8.817165410007025\n",
      "(26600) loss - 8.817165410007025\n",
      "(26700) loss - 8.817165410007025\n",
      "(26800) loss - 8.817165410007025\n",
      "(26900) loss - 8.817165410007025\n",
      "(27000) loss - 8.817165410007025\n",
      "(27100) loss - 8.817165410007025\n",
      "(27200) loss - 8.817165410007025\n",
      "(27300) loss - 8.817165410007025\n",
      "(27400) loss - 8.817165410007025\n",
      "(27500) loss - 8.817165410007025\n",
      "(27600) loss - 8.817165410007025\n",
      "(27700) loss - 8.817165410007025\n",
      "(27800) loss - 8.817165410007025\n",
      "(27900) loss - 8.817165410007025\n",
      "(28000) loss - 8.817165410007025\n",
      "(28100) loss - 8.817165410007025\n",
      "(28200) loss - 8.817165410007025\n",
      "(28300) loss - 8.817165410007025\n",
      "(28400) loss - 8.817165410007025\n",
      "(28500) loss - 8.817165410007025\n",
      "(28600) loss - 8.817165410007025\n",
      "(28700) loss - 8.817165410007025\n",
      "(28800) loss - 8.817165410007025\n",
      "(28900) loss - 8.817165410007025\n",
      "(29000) loss - 8.817165410007025\n",
      "(29100) loss - 8.817165410007025\n",
      "(29200) loss - 8.817165410007025\n",
      "(29300) loss - 8.817165410007025\n",
      "(29400) loss - 8.817165410007025\n",
      "(29500) loss - 8.817165410007025\n",
      "(29600) loss - 8.817165410007025\n",
      "(29700) loss - 8.817165410007025\n",
      "(29800) loss - 8.817165410007025\n",
      "(29900) loss - 8.817165410007025\n",
      "(30000) loss - 8.817165410007025\n",
      "(30100) loss - 8.817165410007025\n",
      "(30200) loss - 8.817165410007025\n",
      "(30300) loss - 8.817165410007025\n",
      "(30400) loss - 8.817165410007025\n",
      "(30500) loss - 8.817165410007025\n",
      "(30600) loss - 8.817165410007025\n",
      "(30700) loss - 8.817165410007025\n",
      "(30800) loss - 8.817165410007025\n",
      "(30900) loss - 8.817165410007025\n",
      "(31000) loss - 8.817165410007025\n",
      "(31100) loss - 8.817165410007025\n",
      "(31200) loss - 8.817165410007025\n",
      "(31300) loss - 8.817165410007025\n",
      "(31400) loss - 8.817165410007025\n",
      "(31500) loss - 8.817165410007025\n",
      "(31600) loss - 8.817165410007025\n",
      "(31700) loss - 8.817165410007025\n",
      "(31800) loss - 8.817165410007025\n",
      "(31900) loss - 8.817165410007025\n",
      "(32000) loss - 8.817165410007025\n",
      "(32100) loss - 8.817165410007025\n",
      "(32200) loss - 8.817165410007025\n",
      "(32300) loss - 8.817165410007025\n",
      "(32400) loss - 8.817165410007025\n",
      "(32500) loss - 8.817165410007025\n",
      "(32600) loss - 8.817165410007025\n",
      "(32700) loss - 8.817165410007025\n",
      "(32800) loss - 8.817165410007025\n",
      "(32900) loss - 8.817165410007025\n",
      "(33000) loss - 8.817165410007025\n",
      "(33100) loss - 8.817165410007025\n",
      "(33200) loss - 8.817165410007025\n",
      "(33300) loss - 8.817165410007025\n",
      "(33400) loss - 8.817165410007025\n",
      "(33500) loss - 8.817165410007025\n",
      "(33600) loss - 8.817165410007025\n",
      "(33700) loss - 8.817165410007025\n",
      "(33800) loss - 8.817165410007025\n",
      "(33900) loss - 8.817165410007025\n",
      "(34000) loss - 8.817165410007025\n",
      "(34100) loss - 8.817165410007025\n",
      "(34200) loss - 8.817165410007025\n",
      "(34300) loss - 8.817165410007025\n",
      "(34400) loss - 8.817165410007025\n",
      "(34500) loss - 8.817165410007025\n",
      "(34600) loss - 8.817165410007025\n",
      "(34700) loss - 8.817165410007025\n",
      "(34800) loss - 8.817165410007025\n",
      "(34900) loss - 8.817165410007025\n",
      "(35000) loss - 8.817165410007025\n",
      "(35100) loss - 8.817165410007025\n",
      "(35200) loss - 8.817165410007025\n",
      "(35300) loss - 8.817165410007025\n",
      "(35400) loss - 8.817165410007025\n",
      "(35500) loss - 8.817165410007025\n",
      "(35600) loss - 8.817165410007025\n",
      "(35700) loss - 8.817165410007025\n",
      "(35800) loss - 8.817165410007025\n",
      "(35900) loss - 8.817165410007025\n",
      "(36000) loss - 8.817165410007025\n",
      "(36100) loss - 8.817165410007025\n",
      "(36200) loss - 8.817165410007025\n",
      "(36300) loss - 8.817165410007025\n",
      "(36400) loss - 8.817165410007025\n",
      "(36500) loss - 8.817165410007025\n",
      "(36600) loss - 8.817165410007025\n",
      "(36700) loss - 8.817165410007025\n",
      "(36800) loss - 8.817165410007025\n",
      "(36900) loss - 8.817165410007025\n",
      "(37000) loss - 8.817165410007025\n",
      "(37100) loss - 8.817165410007025\n",
      "(37200) loss - 8.817165410007025\n",
      "(37300) loss - 8.817165410007025\n",
      "(37400) loss - 8.817165410007025\n",
      "(37500) loss - 8.817165410007025\n",
      "(37600) loss - 8.817165410007025\n",
      "(37700) loss - 8.817165410007025\n",
      "(37800) loss - 8.817165410007025\n",
      "(37900) loss - 8.817165410007025\n",
      "(38000) loss - 8.817165410007025\n",
      "(38100) loss - 8.817165410007025\n",
      "(38200) loss - 8.817165410007025\n",
      "(38300) loss - 8.817165410007025\n",
      "(38400) loss - 8.817165410007025\n",
      "(38500) loss - 8.817165410007025\n",
      "(38600) loss - 8.817165410007025\n",
      "(38700) loss - 8.817165410007025\n",
      "(38800) loss - 8.817165410007025\n",
      "(38900) loss - 8.817165410007025\n",
      "(39000) loss - 8.817165410007025\n",
      "(39100) loss - 8.817165410007025\n",
      "(39200) loss - 8.817165410007025\n",
      "(39300) loss - 8.817165410007025\n",
      "(39400) loss - 8.817165410007025\n",
      "(39500) loss - 8.817165410007025\n",
      "(39600) loss - 8.817165410007025\n",
      "(39700) loss - 8.817165410007025\n",
      "(39800) loss - 8.817165410007025\n",
      "(39900) loss - 8.817165410007025\n",
      "(40000) loss - 8.817165410007025\n",
      "(40100) loss - 8.817165410007025\n",
      "(40200) loss - 8.817165410007025\n",
      "(40300) loss - 8.817165410007025\n",
      "(40400) loss - 8.817165410007025\n",
      "(40500) loss - 8.817165410007025\n",
      "(40600) loss - 8.817165410007025\n",
      "(40700) loss - 8.817165410007025\n",
      "(40800) loss - 8.817165410007025\n",
      "(40900) loss - 8.817165410007025\n",
      "(41000) loss - 8.817165410007025\n",
      "(41100) loss - 8.817165410007025\n",
      "(41200) loss - 8.817165410007025\n",
      "(41300) loss - 8.817165410007025\n",
      "(41400) loss - 8.817165410007025\n",
      "(41500) loss - 8.817165410007025\n",
      "(41600) loss - 8.817165410007025\n",
      "(41700) loss - 8.817165410007025\n",
      "(41800) loss - 8.817165410007025\n",
      "(41900) loss - 8.817165410007025\n",
      "(42000) loss - 8.817165410007025\n",
      "(42100) loss - 8.817165410007025\n",
      "(42200) loss - 8.817165410007025\n",
      "(42300) loss - 8.817165410007025\n",
      "(42400) loss - 8.817165410007025\n",
      "(42500) loss - 8.817165410007025\n",
      "(42600) loss - 8.817165410007025\n",
      "(42700) loss - 8.817165410007025\n",
      "(42800) loss - 8.817165410007025\n",
      "(42900) loss - 8.817165410007025\n",
      "(43000) loss - 8.817165410007025\n",
      "(43100) loss - 8.817165410007025\n",
      "(43200) loss - 8.817165410007025\n",
      "(43300) loss - 8.817165410007025\n",
      "(43400) loss - 8.817165410007025\n",
      "(43500) loss - 8.817165410007025\n",
      "(43600) loss - 8.817165410007025\n",
      "(43700) loss - 8.817165410007025\n",
      "(43800) loss - 8.817165410007025\n",
      "(43900) loss - 8.817165410007025\n",
      "(44000) loss - 8.817165410007025\n",
      "(44100) loss - 8.817165410007025\n",
      "(44200) loss - 8.817165410007025\n",
      "(44300) loss - 8.817165410007025\n",
      "(44400) loss - 8.817165410007025\n",
      "(44500) loss - 8.817165410007025\n",
      "(44600) loss - 8.817165410007025\n",
      "(44700) loss - 8.817165410007025\n",
      "(44800) loss - 8.817165410007025\n",
      "(44900) loss - 8.817165410007025\n",
      "(45000) loss - 8.817165410007025\n",
      "(45100) loss - 8.817165410007025\n",
      "(45200) loss - 8.817165410007025\n",
      "(45300) loss - 8.817165410007025\n",
      "(45400) loss - 8.817165410007025\n",
      "(45500) loss - 8.817165410007025\n",
      "(45600) loss - 8.817165410007025\n",
      "(45700) loss - 8.817165410007025\n",
      "(45800) loss - 8.817165410007025\n",
      "(45900) loss - 8.817165410007025\n",
      "(46000) loss - 8.817165410007025\n",
      "(46100) loss - 8.817165410007025\n",
      "(46200) loss - 8.817165410007025\n",
      "(46300) loss - 8.817165410007025\n",
      "(46400) loss - 8.817165410007025\n",
      "(46500) loss - 8.817165410007025\n",
      "(46600) loss - 8.817165410007025\n",
      "(46700) loss - 8.817165410007025\n",
      "(46800) loss - 8.817165410007025\n",
      "(46900) loss - 8.817165410007025\n",
      "(47000) loss - 8.817165410007025\n",
      "(47100) loss - 8.817165410007025\n",
      "(47200) loss - 8.817165410007025\n",
      "(47300) loss - 8.817165410007025\n",
      "(47400) loss - 8.817165410007025\n",
      "(47500) loss - 8.817165410007025\n",
      "(47600) loss - 8.817165410007025\n",
      "(47700) loss - 8.817165410007025\n",
      "(47800) loss - 8.817165410007025\n",
      "(47900) loss - 8.817165410007025\n",
      "(48000) loss - 8.817165410007025\n",
      "(48100) loss - 8.817165410007025\n",
      "(48200) loss - 8.817165410007025\n",
      "(48300) loss - 8.817165410007025\n",
      "(48400) loss - 8.817165410007025\n",
      "(48500) loss - 8.817165410007025\n",
      "(48600) loss - 8.817165410007025\n",
      "(48700) loss - 8.817165410007025\n",
      "(48800) loss - 8.817165410007025\n",
      "(48900) loss - 8.817165410007025\n",
      "(49000) loss - 8.817165410007025\n",
      "(49100) loss - 8.817165410007025\n",
      "(49200) loss - 8.817165410007025\n",
      "(49300) loss - 8.817165410007025\n",
      "(49400) loss - 8.817165410007025\n",
      "(49500) loss - 8.817165410007025\n",
      "(49600) loss - 8.817165410007025\n",
      "(49700) loss - 8.817165410007025\n",
      "(49800) loss - 8.817165410007025\n",
      "(49900) loss - 8.817165410007025\n",
      "Test sin(0) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(0): -1.2246467991473532e-16 0.2022971266418936\n",
      "Test sin(1) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(1): -0.0031431590602383898 0.19631286458346198\n",
      "Test sin(2) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(2): -0.006286287067720267 0.19034596547918303\n",
      "Test sin(3) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(3): -0.009429352969997477 0.18439641194001766\n",
      "Test sin(4) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(4): -0.01257232571523354 0.17846418657692897\n",
      "Test sin(5) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(5): -0.015715174252514088 0.17254927200087966\n",
      "Test sin(6) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(6): -0.018857867532150532 0.16665165082283018\n",
      "Test sin(7) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(7): -0.022000374505988154 0.16077130565374587\n",
      "Test sin(8) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(8): -0.02514266412771416 0.15490821910458719\n",
      "Test sin(9) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(9): -0.028284705353161754 0.14906237378631548\n",
      "Test sin(10) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(10): -0.03142646714061816 0.1432337523098952\n",
      "Test sin(11) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(11): -0.03456791845113265 0.1374223372862864\n",
      "Test sin(12) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(12): -0.037709028248820047 0.1316281113264539\n",
      "Test sin(13) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(13): -0.04084976550117093 0.1258510570413578\n",
      "Test sin(14) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(14): -0.043990099179355084 0.12009115704196116\n",
      "Test sin(15) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(15): -0.04712999825852942 0.11434839393922758\n",
      "Test sin(16) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(16): -0.05026943171814577 0.10862275034411573\n",
      "Test sin(17) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(17): -0.05340836854225426 0.10291420886759273\n",
      "Test sin(18) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(18): -0.05654677771981331 0.09722275212061726\n",
      "Test sin(19) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(19): -0.05968462824499287 0.09154836271415245\n",
      "Test sin(20) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(20): -0.06282188911748207 0.08589102325916187\n",
      "Test sin(21) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(21): -0.06595852934279685 0.08025071636660641\n",
      "Test sin(22) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(22): -0.06909451793258348 0.07462742464744876\n",
      "Test sin(23) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(23): -0.07222982390492601 0.06902113071265159\n",
      "Test sin(24) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(24): -0.07536441628465378 0.06343181717317581\n",
      "Test sin(25) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(25): -0.07849826410364423 0.05785946663998631\n",
      "Test sin(26) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(26): -0.08163133640113247 0.05230406172404267\n",
      "Test sin(27) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(27): -0.08476360222401401 0.046765585036308455\n",
      "Test sin(28) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(28): -0.08789503062715191 0.04124401918774634\n",
      "Test sin(29) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(29): -0.09102559067368382 0.035739346789318116\n",
      "Test sin(30) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(30): -0.09415525143532452 0.03025155045198602\n",
      "Test sin(31) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(31): -0.09728398199267504 0.024780612786711842\n",
      "Test sin(32) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(32): -0.100411751435525 0.019326516404458705\n",
      "Test sin(33) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(33): -0.1035385288631593 0.013889243916189287\n",
      "Test sin(34) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(34): -0.10666428338466481 0.00846877793286449\n",
      "Test sin(35) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(35): -0.1097889841192328 0.0030651010654469957\n",
      "Test sin(36) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(36): -0.11291260019646542 -0.0023218040750996316\n",
      "Test sin(37) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(37): -0.11603510075668197 -0.007691954877814489\n",
      "Test sin(38) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(38): -0.11915645495122074 -0.013045368731734008\n",
      "Test sin(39) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(39): -0.12227663194274724 -0.018382063025897732\n",
      "Test sin(40) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(40): -0.12539560090555582 -0.023702055149342538\n",
      "Test sin(41) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(41): -0.12851333102587553 -0.029005362491105302\n",
      "Test sin(42) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(42): -0.13162979150217582 -0.03429200244022468\n",
      "Test sin(43) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(43): -0.1347449515454678 -0.0395619923857371\n",
      "Test sin(44) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(44): -0.13785878037961197 -0.04481534971668255\n",
      "Test sin(45) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(45): -0.14097124724161908 -0.050052091822097466\n",
      "Test sin(46) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(46): -0.14408232138195554 -0.05527223609101828\n",
      "Test sin(47) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(47): -0.14719197206484838 -0.06047579991248497\n",
      "Test sin(48) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(48): -0.15030016856858633 -0.06566280067553443\n",
      "Test sin(49) (<function Weight.<lambda> at 0x7f00b3f1a950> * 0.17285080561055646 + <function sinus_polynom.<locals>.<lambda> at 0x7f00e07df5b0> * 0.683890037463221 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca050> * -2.055004287913967e-17 + <function sinus_polynom.<locals>.<lambda> at 0x7f009dcca170> * -0.09333038904059784)(49): -0.15340688018582463 -0.07083325576920307\n"
     ]
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "from typing import Callable\n",
    "# \n",
    "class Weight:\n",
    "    \"\"\"\n",
    "    Class representing single variable\n",
    "    \"\"\"\n",
    "    def __init__(self, value: float, coefficient: Callable[[NDArray], NDArray] = lambda x: x) -> None:\n",
    "        self.grad = 0\n",
    "        self.coefficient = coefficient\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, values: NDArray) -> NDArray:\n",
    "        return self.value * self.coefficient(values)\n",
    "\n",
    "    def compute_grad(self, X: NDArray, loss):\n",
    "        self.grad = (self.coefficient(X) * loss).sum()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.coefficient} * {self.value}'\n",
    "\n",
    "    # backward will be needed for neural networs \n",
    "\n",
    "\n",
    "class Approximator:\n",
    "    def __init__(self, weights: list[Weight]) -> None:\n",
    "        self.weights = weights\n",
    "\n",
    "    def __call__(self, values: NDArray) -> NDArray:\n",
    "        a = np.sum([w(values) for w in self.weights], axis=0) # Summing up all weights in polynomial\n",
    "        return a \n",
    "        \n",
    "\n",
    "    def update_weights(self, update_step: float, X: NDArray, loss_grad: float):\n",
    "        for w in self.weights:\n",
    "            w.compute_grad(X, loss_grad)\n",
    "            w.value -= update_step * w.grad \n",
    "\n",
    "    def __add__(self, weight: Weight):\n",
    "        self.weights.append(weight)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return ' + '.join([str(w) for w in self.weights])\n",
    "\n",
    "\n",
    "def sinus_polynom():\n",
    "    return Approximator([\n",
    "        Weight(np.random.rand()),\n",
    "        Weight(np.random.rand(), lambda x: np.power(x, 1)),\n",
    "        Weight(np.random.rand(), lambda x: np.power(x, 2)),\n",
    "        Weight(np.random.rand(), lambda x: np.power(x, 3))\n",
    "    ])\n",
    "    \n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "alpha = 1e-6\n",
    "def train(approximator: Approximator, X: NDArray, Y: NDArray, n_steps: int):\n",
    "    for s in range(n_steps):\n",
    "        Y_approx = approximator(X)\n",
    "        loss = np.square(Y - Y_approx).sum() \n",
    "        if s % 100 == 0:\n",
    "            print(f'({s}) loss - {loss}')\n",
    "        \n",
    "        loss_grad = 2 * (Y_approx - Y)\n",
    "        approximator.update_weights(alpha, X, loss_grad)\n",
    "\n",
    "# ---------------------------------\n",
    "X = np.linspace(-pi, pi, num=2000)\n",
    "y = np.sin(X)\n",
    "approximator = sinus_polynom()\n",
    "train(approximator, X, y, 50000)\n",
    "for i, x in enumerate(X[:50]):\n",
    "    print(f'Test sin({i}) ({approximator})({i}): {np.sin(x)} {approximator(x)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. \n",
    "It works, and I think you've got an idea, but you can easyli see, that approximations given by so simple polynomial are not very good. \n",
    "Let's try to make it better and create a Neural Network and make next iteration over our previous example! \n",
    "\n",
    "\n",
    "Basically neural network is a function that is a combination of layers. \n",
    "So, in order to create a network we should do next things: \n",
    "1. Implement logic of backward propagation\n",
    "2. Implement dot product gradient (because weights are multiplied between layers)\n",
    "3. Create layers of weights and after that connect them all (basically it would be an extension of an Approximator class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - po spotkaniu\n",
    "\n",
    "1. wizualizacja funkcji straty sinusu w zależności od wag. Ma to taką zaletę, że student zobaczy jaki wpływ mają wagi na loss: F(W) -> loss \n",
    "2. chihuahua i muffinki\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Make simple neural network based on `Approximators` to predict sinus value\n",
    "2. Refactor code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "from typing import Callable\n",
    "\n",
    "# In this step we will train without an activation function and we will see,\n",
    "# that the training is very unefficient, because of very big changes in value.\n",
    "# And in the next cell we will intorduce the cruicial part -- activation functions. \n",
    "# And basically it would show all nessesary components for having a Neural Network.\n",
    "\n",
    "class MultivariateApproximator:\n",
    "    def __init__(self, initial_weights: NDArray) -> None:\n",
    "        \"\"\"\n",
    "        :param initial_weights: N-dimensional vector\n",
    "        \"\"\"\n",
    "        self._weights = initial_weights\n",
    "\n",
    "    def __call__(self, variables: NDArray) -> NDArray:\n",
    "        \"\"\"\n",
    "        :param variables: M x N variables values\n",
    "        :returns: M-dimensional vector\n",
    "\n",
    "        \"\"\"\n",
    "        return self._weights @ variables\n",
    "\n",
    "    def _update_weights(self, X: NDArray, loss: NDArray):\n",
    "        \"\"\"\n",
    "        :param X: values for each loss was calculated\n",
    "        :param loss: loss value for every input\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Basically, here I should create a Jacobian of Xs \n",
    "        multiplied by loss. \n",
    "        Am I right? \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layers: list[list[MultivariateApproximator]]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def fit(self, X: NDArray, Y: NDArray, n_steps: int):\n",
    "        for s in range(n_steps):\n",
    "            Y_approx = self.predict(X)\n",
    "            loss = np.square(Y - Y_approx).sum()\n",
    "            if s % 100 == 0:\n",
    "                print(f'({s}) loss - {loss}')\n",
    "            \n",
    "            loss_grad = 2 * (Y_approx - Y)\n",
    "        \n",
    "    def _backpropagate(self, X: NDArray, grad: NDArray):\n",
    "        ...\n",
    "        \n",
    "\n",
    "    def predict(self, X: NDArray):\n",
    "        ...\n",
    "    \n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "alpha = 1e-6\n",
    "def train(approximator: Approximator, X: NDArray, Y: NDArray, n_steps: int):\n",
    "    for s in range(n_steps):\n",
    "        Y_approx = approximator(X)\n",
    "        loss = np.square(Y - Y_approx).sum() \n",
    "        if s % 100 == 0:\n",
    "            print(f'({s}) loss - {loss}')\n",
    "        \n",
    "        loss_grad = 2 * (Y_approx - Y)\n",
    "        approximator.update_weights(alpha, X, loss_grad)\n",
    "\n",
    "# ---------------------------------\n",
    "X = np.linspace(-pi, pi, num=2000)\n",
    "y = np.sin(X)\n",
    "approximator = sinus_polynom()\n",
    "train(approximator, X, y, 50000)\n",
    "for i, x in enumerate(X[:50]):\n",
    "    print(f'Test sin({i}) ({approximator})({i}): {np.sin(x)} {approximator(x)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ai-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82fd92b5e2d525eb1997b978c0e983ce3f887f1794c79a067113c708b45f4fb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
